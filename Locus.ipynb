{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yo this notebook is lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage.io import imshow\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "y_train_labels = [class_labels[int(val)] for val in y_train]\n",
    "h = 32\n",
    "w = 32\n",
    "\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0\n",
    "X_train = X_train - 1/2\n",
    "X_test = X_test - 1/2\n",
    "\n",
    "# Takes only the classes we are concerned with \n",
    "sub_sample = [1, 4, 9]\n",
    "X_sub_train = []\n",
    "y_sub_train = []\n",
    "for i in range(0,len(y_train)):\n",
    "    if y_train[i][0] in sub_sample:\n",
    "        X_sub_train.append(X_train[i])\n",
    "        if y_train[i][0] == 1 or y_train[i][0] == 9:\n",
    "            y_sub_train.append([0])\n",
    "        if y_train[i][0] == 4:\n",
    "            y_sub_train.append([1])\n",
    "\n",
    "        \n",
    "X_sub_test = []\n",
    "y_sub_test = []\n",
    "for i in range(0,len(y_test)):\n",
    "    if y_test[i][0] in sub_sample:\n",
    "        X_sub_test.append(X_test[i])\n",
    "        # If it's a truck or automobile\n",
    "        if y_test[i][0] == 1 or y_test[i][0] == 9:\n",
    "            y_sub_test.append([0])\n",
    "        # If it's a deer\n",
    "        if y_test[i][0] == 4:\n",
    "            y_sub_test.append([1])\n",
    "\n",
    "X_sub_train = np.array(X_sub_train)\n",
    "X_sub_test = np.array(X_sub_test)\n",
    "y_sub_train = np.array(y_sub_train)\n",
    "y_sub_test = np.array(y_sub_test)\n",
    "        \n",
    "sub_class_labels = ['automobile', 'deer']\n",
    "y_sub_train_labels = [sub_class_labels[int(val)] for val in y_sub_train]\n",
    "\n",
    "NUM_CLASSES = len(sub_class_labels)\n",
    "y_sub_train_ohe = keras.utils.to_categorical(y_sub_train, NUM_CLASSES)\n",
    "y_sub_test_ohe = keras.utils.to_categorical(y_sub_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(X_sub_train)\n",
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11c532cf8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imshow(X_sub_train[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "15000\n",
      "3000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_sub_train))\n",
    "print(len(y_sub_train))\n",
    "print(len(X_sub_test))\n",
    "print(len(y_sub_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_c = np.concatenate([X_train,X_test], axis=0)\n",
    "#y_c = np.concatenate([y_train,y_test], axis=0).flatten()\n",
    "#vectorized_color = np.reshape(X_c,(len(X_c),32*32,3))\n",
    "#color_pixels = np.reshape(vectorized_color,(len(vectorized_color)*32*32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bw_pixels = [(0.21*pixel[0])+(0.72*pixel[1])+(0.07*pixel[2]) for pixel in color_pixels]#convert rgb to b/w\n",
    "#vectorized_bw = np.reshape(bw_pixels,(len(X),32*32))#reconstruct to list in image vectors\n",
    "#vectorized_bw.dump(\"data/vectorized_bw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vectorized_bw_all = np.load(\"data/vectorized_bw\")\\nprint(vectorized_bw_all.shape)\\n\\n#we might need these DF\\'s later\\ndf_bw = pd.DataFrame(data=vectorized_bw_all)\\ndf_bw = df_bw.assign(y=pd.Series(y,name=\\'y\\'))\\n#df_bw_sample = df_bw.sample(5000)\\nX = df_bw.drop(\\'y\\', axis=1).as_matrix()\\ny = df_bw.y.as_matrix()\\nprint(X.shape)\\nprint(y.shape)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''vectorized_bw_all = np.load(\"data/vectorized_bw\")\n",
    "print(vectorized_bw_all.shape)\n",
    "\n",
    "#we might need these DF's later\n",
    "df_bw = pd.DataFrame(data=vectorized_bw_all)\n",
    "df_bw = df_bw.assign(y=pd.Series(y,name='y'))\n",
    "#df_bw_sample = df_bw.sample(5000)\n",
    "X = df_bw.drop('y', axis=1).as_matrix()\n",
    "y = df_bw.y.as_matrix()\n",
    "print(X.shape)\n",
    "print(y.shape)'''\n",
    "#now, we can do cv or a train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import average \n",
    "from keras.models import Input, Model\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=5, # used, Int. Degree range for random rotations.\n",
    "    width_shift_range=0.1, # used, Float (fraction of total width). Range for random horizontal shifts.\n",
    "    height_shift_range=0.1, # used,  Float (fraction of total height). Range for random vertical shifts.\n",
    "    shear_range=0., # Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
    "    zoom_range=0.,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=None)\n",
    "\n",
    "datagen.fit(X_sub_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 61/117 [==============>...............] - ETA: 18s - loss: -0.9697 - acc: 0.3381"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-1d32af9a3457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nfrom keras.layers import average \\nfrom keras.models import Input, Model\\nfrom keras.callbacks import EarlyStopping\\nfrom keras.regularizers import l2 \\n\\nl2_lambda = 0.0001\\n\\nnum_ensembles = 3\\n\\ninput_holder = Input(shape=(w, h, 3))\\n\\nbranches = []\\nfor _ in range(num_ensembles):\\n\\n    conv1 = Conv2D(filters=32,\\n                   input_shape = (w,h,1),\\n                   kernel_size=(3,3),\\n                   kernel_initializer=\\'he_uniform\\', \\n                   kernel_regularizer=l2(l2_lambda),\\n                   padding=\\'same\\', \\n                   activation=\\'relu\\')(input_holder)\\n    \\n    max1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv1)\\n\\n    conv2 = Conv2D(filters=32,\\n                   kernel_size=(3,3),\\n                   kernel_initializer=\\'he_uniform\\', \\n                   kernel_regularizer=l2(l2_lambda),\\n                   padding=\\'same\\', \\n                   activation=\\'relu\\')(max1)\\n    \\n    max2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv2)\\n\\n\\n    # add one layer on flattened output\\n    drop1 = Dropout(0.25)(max2) # add some dropout for regularization after conv layers\\n    flat1 = Flatten()(drop1)\\n    dense1 = Dense(128, \\n                  activation=\\'relu\\',\\n                  kernel_initializer=\\'he_uniform\\',\\n                  kernel_regularizer=l2(l2_lambda)\\n                )(flat1)\\n    drop2 = Dropout(0.5)(dense1) # add some dropout for regularization, again!\\n    dense2 = Dense(NUM_CLASSES, \\n                  activation=\\'sigmoid\\', \\n                  kernel_initializer=\\'glorot_uniform\\',\\n                  kernel_regularizer=l2(l2_lambda)\\n                 )(drop2)\\n    \\n    # now add this branch onto the master list\\n    branches.append(dense2)\\n\\n# that\\'s it, we just need to average the results\\nave = average(branches)\\n\\n# here is the secret sauce for setting the network using the \\n#   Model API:\\ncnn_ens = Model(inputs=input_holder,outputs=ave)\\n\\n# Let\\'s train the model \\nfrom keras import backend as K\\nimport functools\\nfrom itertools import product\\nimport tensorflow as tf\\n# weights=np.array([[1,0],[-1,1]])\\ndef w_categorical_crossentropy(y_true, y_pred):\\n    weights=np.array([[0.1,-0.1],[-0.1,0.1]])\\n    nb_cl = len(weights)\\n    final_mask = K.zeros_like(y_pred[:, 0])\\n    y_pred_max = K.max(y_pred, axis=1)\\n    y_pred_max = K.expand_dims(y_pred_max, 1)\\n    y_pred_max_mat = K.equal(y_pred, y_pred_max)\\n    for c_p, c_t in product(range(nb_cl), range(nb_cl)):\\n\\n        final_mask += (K.cast(weights[c_t, c_p],K.floatx()) * K.cast(y_pred_max_mat[:, c_p] ,K.floatx())* K.cast(y_true[:, c_t],K.floatx()))\\n    return K.categorical_crossentropy(y_pred, y_true) * final_mask\\n\\ncnn_ens.compile(loss=w_categorical_crossentropy, # \\'categorical_crossentropy\\' \\'mean_squared_error\\'\\n                optimizer=\\'rmsprop\\', # \\'adadelta\\' \\'rmsprop\\'\\n                metrics=[\\'accuracy\\'])\\n\\n# the flow method yields batches of images indefinitely, with the given transofmrations\\ncnn_ens.fit_generator(datagen.flow(X_sub_train, y_sub_train_ohe, batch_size=128), \\n                      steps_per_epoch=int(len(X_sub_train)/128), # how many generators to go through per epoch\\n                      epochs=2, verbose=1,\\n                      validation_data=(X_sub_test,y_sub_test_ohe),\\n                      callbacks=[EarlyStopping(monitor=\\'val_loss\\', patience=4)]\\n                     )'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2042\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers import average \n",
    "from keras.models import Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "\n",
    "l2_lambda = 0.0001\n",
    "\n",
    "num_ensembles = 3\n",
    "\n",
    "input_holder = Input(shape=(w, h, 3))\n",
    "\n",
    "branches = []\n",
    "for _ in range(num_ensembles):\n",
    "\n",
    "    conv1 = Conv2D(filters=32,\n",
    "                   input_shape = (w,h,1),\n",
    "                   kernel_size=(3,3),\n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda),\n",
    "                   padding='same', \n",
    "                   activation='relu')(input_holder)\n",
    "    \n",
    "    max1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv1)\n",
    "\n",
    "    conv2 = Conv2D(filters=32,\n",
    "                   kernel_size=(3,3),\n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda),\n",
    "                   padding='same', \n",
    "                   activation='relu')(max1)\n",
    "    \n",
    "    max2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv2)\n",
    "\n",
    "\n",
    "    # add one layer on flattened output\n",
    "    drop1 = Dropout(0.25)(max2) # add some dropout for regularization after conv layers\n",
    "    flat1 = Flatten()(drop1)\n",
    "    dense1 = Dense(128, \n",
    "                  activation='relu',\n",
    "                  kernel_initializer='he_uniform',\n",
    "                  kernel_regularizer=l2(l2_lambda)\n",
    "                )(flat1)\n",
    "    drop2 = Dropout(0.5)(dense1) # add some dropout for regularization, again!\n",
    "    dense2 = Dense(NUM_CLASSES, \n",
    "                  activation='sigmoid', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  kernel_regularizer=l2(l2_lambda)\n",
    "                 )(drop2)\n",
    "    \n",
    "    # now add this branch onto the master list\n",
    "    branches.append(dense2)\n",
    "\n",
    "# that's it, we just need to average the results\n",
    "ave = average(branches)\n",
    "\n",
    "# here is the secret sauce for setting the network using the \n",
    "#   Model API:\n",
    "cnn_ens = Model(inputs=input_holder,outputs=ave)\n",
    "\n",
    "# Let's train the model \n",
    "from keras import backend as K\n",
    "import functools\n",
    "from itertools import product\n",
    "import tensorflow as tf\n",
    "# weights=np.array([[1,0],[-1,1]])\n",
    "def w_categorical_crossentropy(y_true, y_pred):\n",
    "    weights=np.array([[0.1,-0.1],[-0.1,0.1]])\n",
    "    nb_cl = len(weights)\n",
    "    final_mask = K.zeros_like(y_pred[:, 0])\n",
    "    y_pred_max = K.max(y_pred, axis=1)\n",
    "    y_pred_max = K.expand_dims(y_pred_max, 1)\n",
    "    y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "    for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "\n",
    "        final_mask += (K.cast(weights[c_t, c_p],K.floatx()) * K.cast(y_pred_max_mat[:, c_p] ,K.floatx())* K.cast(y_true[:, c_t],K.floatx()))\n",
    "    return K.categorical_crossentropy(y_pred, y_true) * final_mask\n",
    "\n",
    "cnn_ens.compile(loss=w_categorical_crossentropy, # 'categorical_crossentropy' 'mean_squared_error'\n",
    "                optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# the flow method yields batches of images indefinitely, with the given transofmrations\n",
    "cnn_ens.fit_generator(datagen.flow(X_sub_train, y_sub_train_ohe, batch_size=128), \n",
    "                      steps_per_epoch=int(len(X_sub_train)/128), # how many generators to go through per epoch\n",
    "                      epochs=2, verbose=1,\n",
    "                      validation_data=(X_sub_test,y_sub_test_ohe),\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=4)]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.417006274264\n",
      "Accuracy:  0.474\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "yhat_cnn = np.argmax(cnn_ens.predict(X_sub_test), axis=1)\n",
    "rec_cnn = mt.f1_score(y_sub_test,yhat_cnn, average='macro')\n",
    "acc_cnn = mt.accuracy_score(y_sub_test,yhat_cnn)\n",
    "print(\"Recall: \", rec_cnn)\n",
    "print(\"Accuracy: \", acc_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for a in yhat_cnn:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "l2_lambda = 0.0001\n",
    "img_wh = 32\n",
    "\n",
    "# Use Kaiming He to regularize ReLU layers: https://arxiv.org/pdf/1502.01852.pdf\n",
    "# Use Glorot/Bengio for linear/sigmoid/softmax: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf \n",
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Conv2D(filters=32,\n",
    "               input_shape = (img_wh,img_wh,1),\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu')) # more compact syntax\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "\n",
    "cnn.add(Conv2D(filters=32,\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "    \n",
    "\n",
    "# add one layer on flattened output\n",
    "cnn.add(Dropout(0.25)) # add some dropout for regularization after conv layers\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, \n",
    "              activation='relu',\n",
    "              kernel_initializer='he_uniform',\n",
    "              kernel_regularizer=l2(l2_lambda)\n",
    "       ))\n",
    "cnn.add(Dropout(0.5)) # add some dropout for regularization, again!\n",
    "cnn.add(Dense(NUM_CLASSES, \n",
    "              activation='softmax', \n",
    "              kernel_initializer='glorot_uniform',\n",
    "              kernel_regularizer=l2(l2_lambda)\n",
    "             ))\n",
    "\n",
    "# Let's train the model \n",
    "cnn.compile(loss='categorical_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "              optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# the flow method yields batches of images indefinitely, with the given transofmrations\n",
    "cnn.fit_generator(datagen.flow(X_subtrain, y_train_ohe, batch_size=128), \n",
    "                  steps_per_epoch=int(len(X_train)/128), # how many generators to go through per epoch\n",
    "                  epochs=50, verbose=1,\n",
    "                  validation_data=(X_sub_test,y_test_ohe),\n",
    "                  callbacks=[EarlyStopping(monitor='val_loss', patience=2)]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BC for Recall Score Evaluation\n",
    "Recall scores penalizes the model for predicting false negatives. Our data encoding sets cars/trucks to be equal to 0 (negative class) and deer to be 1 (positive class). Thus, the recall metric scores the model pooly if it clasifies a deer (1) as a car (0). Since the business exists with the goal of reducing automobile collisions with deer, it is important that we do not assure mororists that there are no deer (only cars) on the road ahead, when in fact there are deer on the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BC for binary_crossentropy\n",
    "We are using binary_crossentropy as our loss function. Binary classification is the only binary classification metric available to us by Keras. We use this instead of something like mean squared error because this is a classification problem, not a regression one. \n",
    "\n",
    "Ideally, we would write a custom weighted binary crossentropy function, so that the model is only slightly penalized for producing false positives (see case for Recall Score Evaluation), but heavily penalized for producing false negatives. We tried [this implementation](https://github.com/fchollet/keras/issues/2115) from GitHib, but it seems to cause a vanishing gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
