{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yo this notebook is lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from skimage.io import imshow\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "y_train_labels = [class_labels[int(val)] for val in y_train]\n",
    "h = 32\n",
    "w = 32\n",
    "\n",
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0\n",
    "X_train = X_train - 1/2\n",
    "X_test = X_test - 1/2\n",
    "\n",
    "# Takes only the classes we are concerned with \n",
    "sub_sample = [1, 4, 9]\n",
    "X_sub_train = []\n",
    "y_sub_train = []\n",
    "for i in range(0,len(y_train)):\n",
    "    if y_train[i][0] in sub_sample:\n",
    "        X_sub_train.append(X_train[i])\n",
    "        if y_train[i][0] == 1 or y_train[i][0] == 9:\n",
    "            y_sub_train.append([0])\n",
    "        if y_train[i][0] == 4:\n",
    "            y_sub_train.append([1])\n",
    "\n",
    "        \n",
    "X_sub_test = []\n",
    "y_sub_test = []\n",
    "for i in range(0,len(y_test)):\n",
    "    if y_test[i][0] in sub_sample:\n",
    "        X_sub_test.append(X_test[i])\n",
    "        # If it's a truck or automobile\n",
    "        if y_test[i][0] == 1 or y_test[i][0] == 9:\n",
    "            y_sub_test.append([0])\n",
    "        # If it's a deer\n",
    "        if y_test[i][0] == 4:\n",
    "            y_sub_test.append([1])\n",
    "\n",
    "X_sub_train = np.array(X_sub_train)\n",
    "X_sub_test = np.array(X_sub_test)\n",
    "y_sub_train = np.array(y_sub_train)\n",
    "y_sub_test = np.array(y_sub_test)\n",
    "        \n",
    "sub_class_labels = ['automobile', 'deer']\n",
    "y_sub_train_labels = [sub_class_labels[int(val)] for val in y_sub_train]\n",
    "\n",
    "NUM_CLASSES = len(sub_class_labels)\n",
    "y_sub_train_ohe = keras.utils.to_categorical(y_sub_train, NUM_CLASSES)\n",
    "y_sub_test_ohe = keras.utils.to_categorical(y_sub_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_sub_train)\n",
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imshow(X_sub_train[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "15000\n",
      "3000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_sub_train))\n",
    "print(len(y_sub_train))\n",
    "print(len(X_sub_test))\n",
    "print(len(y_sub_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_c = np.concatenate([X_train,X_test], axis=0)\n",
    "#y_c = np.concatenate([y_train,y_test], axis=0).flatten()\n",
    "#vectorized_color = np.reshape(X_c,(len(X_c),32*32,3))\n",
    "#color_pixels = np.reshape(vectorized_color,(len(vectorized_color)*32*32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bw_pixels = [(0.21*pixel[0])+(0.72*pixel[1])+(0.07*pixel[2]) for pixel in color_pixels]#convert rgb to b/w\n",
    "#vectorized_bw = np.reshape(bw_pixels,(len(X),32*32))#reconstruct to list in image vectors\n",
    "#vectorized_bw.dump(\"data/vectorized_bw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vectorized_bw_all = np.load(\"data/vectorized_bw\")\\nprint(vectorized_bw_all.shape)\\n\\n#we might need these DF\\'s later\\ndf_bw = pd.DataFrame(data=vectorized_bw_all)\\ndf_bw = df_bw.assign(y=pd.Series(y,name=\\'y\\'))\\n#df_bw_sample = df_bw.sample(5000)\\nX = df_bw.drop(\\'y\\', axis=1).as_matrix()\\ny = df_bw.y.as_matrix()\\nprint(X.shape)\\nprint(y.shape)'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''vectorized_bw_all = np.load(\"data/vectorized_bw\")\n",
    "print(vectorized_bw_all.shape)\n",
    "\n",
    "#we might need these DF's later\n",
    "df_bw = pd.DataFrame(data=vectorized_bw_all)\n",
    "df_bw = df_bw.assign(y=pd.Series(y,name='y'))\n",
    "#df_bw_sample = df_bw.sample(5000)\n",
    "X = df_bw.drop('y', axis=1).as_matrix()\n",
    "y = df_bw.y.as_matrix()\n",
    "print(X.shape)\n",
    "print(y.shape)'''\n",
    "#now, we can do cv or a train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import average \n",
    "from keras.models import Input, Model\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN #1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s - loss: 0.2708 - acc: 0.9019 - val_loss: 0.2326 - val_acc: 0.9252\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 3s - loss: 0.1602 - acc: 0.9527 - val_loss: 0.1233 - val_acc: 0.9655\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 3s - loss: 0.1335 - acc: 0.9624 - val_loss: 0.1352 - val_acc: 0.9585\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 3s - loss: 0.1227 - acc: 0.9667 - val_loss: 0.1068 - val_acc: 0.9720\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 3s - loss: 0.1151 - acc: 0.9701 - val_loss: 0.1151 - val_acc: 0.9730\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 3s - loss: 0.1061 - acc: 0.9739 - val_loss: 0.0981 - val_acc: 0.9780\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 3s - loss: 0.1017 - acc: 0.9737 - val_loss: 0.1118 - val_acc: 0.9712\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 3s - loss: 0.0945 - acc: 0.9767 - val_loss: 0.0960 - val_acc: 0.9757\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 3s - loss: 0.0958 - acc: 0.9767 - val_loss: 0.0935 - val_acc: 0.9777\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 3s - loss: 0.0918 - acc: 0.9775 - val_loss: 0.0982 - val_acc: 0.9763\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers import average \n",
    "from keras.models import Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "l2_lambda = 0.0001\n",
    "\n",
    "input_holder = Input(shape=(w, h, 3))\n",
    "conv1 = Conv2D(filters=32,\n",
    "               input_shape = (w,h,1),\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu')(input_holder)\n",
    "\n",
    "max1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv1)\n",
    "\n",
    "conv2 = Conv2D(filters=32,\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu')(max1)\n",
    "\n",
    "max2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv2)\n",
    "\n",
    "\n",
    "# add one layer on flattened output\n",
    "drop1 = Dropout(0.25)(max2) # add some dropout for regularization after conv layers\n",
    "flat1 = Flatten()(drop1)\n",
    "dense1 = Dense(128, \n",
    "              activation='relu',\n",
    "              kernel_initializer='he_uniform',\n",
    "              kernel_regularizer=l2(l2_lambda)\n",
    "            )(flat1)\n",
    "drop2 = Dropout(0.5)(dense1) # add some dropout for regularization, again!\n",
    "dense2_1 = Dense(NUM_CLASSES, \n",
    "              activation='sigmoid', \n",
    "              kernel_initializer='glorot_uniform',\n",
    "              kernel_regularizer=l2(l2_lambda)\n",
    "             )(drop2)\n",
    "\n",
    "cnn1 = Model(inputs=input_holder,outputs=dense2_1)\n",
    "\n",
    "# Let's train the model \n",
    "cnn1.compile(loss='binary_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "                optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# the flow method yields batches of images indefinitely, with the given transofmrations\n",
    "cnn1.fit(X_sub_train, y_sub_train_ohe,\n",
    "                      #steps_per_epoch=int(len(X_sub_train)/128), # how many generators to go through per epoch\n",
    "                      epochs=10, verbose=1,\n",
    "                      validation_data=(X_sub_test,y_sub_test_ohe),\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=4)]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.95\n",
      "Accuracy:  0.976666666667\n"
     ]
    }
   ],
   "source": [
    "yhat_cnn = np.argmax(cnn1.predict(X_sub_test), axis=1)\n",
    "rec_cnn = mt.recall_score(y_sub_test,yhat_cnn)\n",
    "acc_cnn = mt.accuracy_score(y_sub_test,yhat_cnn)\n",
    "print(\"Recall: \", rec_cnn)\n",
    "print(\"Accuracy: \", acc_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN #1 with Data Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=5, # used, Int. Degree range for random rotations.\n",
    "    width_shift_range=0.1, # used, Float (fraction of total width). Range for random horizontal shifts.\n",
    "    height_shift_range=0.1, # used,  Float (fraction of total height). Range for random vertical shifts.\n",
    "    shear_range=0., # Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
    "    zoom_range=0.,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "117/117 [==============================] - 6s - loss: 0.1192 - acc: 0.9680 - val_loss: 0.0873 - val_acc: 0.9795\n",
      "Epoch 2/10\n",
      "117/117 [==============================] - 4s - loss: 0.1103 - acc: 0.9706 - val_loss: 0.0859 - val_acc: 0.9797\n",
      "Epoch 3/10\n",
      "117/117 [==============================] - 4s - loss: 0.1040 - acc: 0.9732 - val_loss: 0.0812 - val_acc: 0.9818\n",
      "Epoch 4/10\n",
      "117/117 [==============================] - 4s - loss: 0.1055 - acc: 0.9738 - val_loss: 0.0860 - val_acc: 0.9787\n",
      "Epoch 5/10\n",
      "117/117 [==============================] - 4s - loss: 0.1045 - acc: 0.9719 - val_loss: 0.0936 - val_acc: 0.9742\n",
      "Epoch 6/10\n",
      "117/117 [==============================] - 4s - loss: 0.1006 - acc: 0.9738 - val_loss: 0.0818 - val_acc: 0.9812\n",
      "Epoch 7/10\n",
      "117/117 [==============================] - 4s - loss: 0.0991 - acc: 0.9737 - val_loss: 0.0805 - val_acc: 0.9828\n",
      "Epoch 8/10\n",
      "117/117 [==============================] - 4s - loss: 0.0980 - acc: 0.9755 - val_loss: 0.0904 - val_acc: 0.9803\n",
      "Epoch 9/10\n",
      "117/117 [==============================] - 4s - loss: 0.0960 - acc: 0.9760 - val_loss: 0.0823 - val_acc: 0.9808\n",
      "Epoch 10/10\n",
      "117/117 [==============================] - 4s - loss: 0.0975 - acc: 0.9759 - val_loss: 0.0844 - val_acc: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cf29ea2630>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn1_de = Model(inputs=input_holder,outputs=dense2_1)\n",
    "\n",
    "# Let's train the model \n",
    "cnn1_de.compile(loss='binary_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "                optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "datagen.fit(X_sub_train)\n",
    "cnn1_de.fit_generator(datagen.flow(X_sub_train, y_sub_train_ohe, batch_size=128), \n",
    "                      steps_per_epoch=int(len(X_sub_train)/128), # how many generators to go through per epoch\n",
    "                      epochs=10, verbose=1,\n",
    "                      validation_data=(X_sub_test,y_sub_test_ohe),\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=4)]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.955\n",
      "Accuracy:  0.978333333333\n"
     ]
    }
   ],
   "source": [
    "yhat_cnn = np.argmax(cnn1_de.predict(X_sub_test), axis=1)\n",
    "rec_cnn = mt.recall_score(y_sub_test,yhat_cnn)\n",
    "acc_cnn = mt.accuracy_score(y_sub_test,yhat_cnn)\n",
    "print(\"Recall: \", rec_cnn)\n",
    "print(\"Accuracy: \", acc_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN #2: LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "l2_lambda = 0.0001\n",
    "input_holder = Input(shape=(w, h, 3))\n",
    "\n",
    "conv1 = Conv2D(filters=6,kernel_size=(5,5),\n",
    "               input_shape = (img_wh,img_wh,1), \n",
    "               padding='valid', \n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda))(input_holder)\n",
    "max1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv1)\n",
    "batch1 = BatchNormalization()(max1)\n",
    "activ1 = Activation(\"sigmoid\")(batch1)\n",
    "\n",
    "conv2 = Conv2D(filters=16,kernel_size=(5,5), \n",
    "               padding='valid', \n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda))(activ1)\n",
    "max2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv2)\n",
    "batch2 = BatchNormalization()(max2)\n",
    "activ2 = Activation(\"sigmoid\")(batch2)\n",
    "\n",
    "conv3 = Conv2D(filters=120,kernel_size=(1,1), \n",
    "               padding='valid', \n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda))(activ2)\n",
    "\n",
    "\n",
    "# add one layer on flattened output\n",
    "#drop1 = Dropout(0.25)(max2) # add some dropout for regularization after conv layers\n",
    "flat1 = Flatten()(conv3)\n",
    "dense1 = Dense(128, \n",
    "              activation='relu',\n",
    "              kernel_initializer='he_uniform',\n",
    "              kernel_regularizer=l2(l2_lambda)\n",
    "            )(flat1)\n",
    "drop2 = Dropout(0.5)(dense1) # add some dropout for regularization, again!\n",
    "dense2_2 = Dense(NUM_CLASSES, \n",
    "              activation='sigmoid', \n",
    "              kernel_initializer='glorot_uniform',\n",
    "              kernel_regularizer=l2(l2_lambda)\n",
    "             )(drop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 8s - loss: -5.3369 - acc: 0.6665 - val_loss: -5.3590 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s - loss: -5.3616 - acc: 0.6667 - val_loss: -5.3650 - val_acc: 0.6667\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s - loss: -5.3651 - acc: 0.6667 - val_loss: -5.3681 - val_acc: 0.6667\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s - loss: -5.3676 - acc: 0.6667 - val_loss: -5.3696 - val_acc: 0.6667\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s - loss: -5.3681 - acc: 0.6667 - val_loss: -5.3698 - val_acc: 0.6667\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s - loss: -5.3690 - acc: 0.6667 - val_loss: -5.3706 - val_acc: 0.6667\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s - loss: -5.3697 - acc: 0.6667 - val_loss: -5.3707 - val_acc: 0.6667\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s - loss: -5.3699 - acc: 0.6667 - val_loss: -5.3711 - val_acc: 0.6667\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s - loss: -5.3699 - acc: 0.6667 - val_loss: -5.3711 - val_acc: 0.6667\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s - loss: -5.3702 - acc: 0.6667 - val_loss: -5.3712 - val_acc: 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cf28f1ac88>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn2 = Model(inputs=input_holder,outputs=dense2_2)\n",
    "\n",
    "# Let's train the model \n",
    "cnn2.compile(loss=w_categorical_crossentropy, # 'categorical_crossentropy' 'mean_squared_error'\n",
    "                optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# the flow method yields batches of images indefinitely, with the given transofmrations\n",
    "cnn2.fit(X_sub_train, y_sub_train_ohe,\n",
    "                      #steps_per_epoch=int(len(X_sub_train)/128), # how many generators to go through per epoch\n",
    "                      epochs=10, verbose=1,\n",
    "                      validation_data=(X_sub_test,y_sub_test_ohe),\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=4)]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.904\n",
      "Accuracy:  0.956333333333\n"
     ]
    }
   ],
   "source": [
    "yhat_cnn = np.argmax(cnn2.predict(X_sub_test), axis=1)\n",
    "rec_cnn = mt.recall_score(y_sub_test,yhat_cnn)\n",
    "acc_cnn = mt.accuracy_score(y_sub_test,yhat_cnn)\n",
    "print(\"Recall: \", rec_cnn)\n",
    "print(\"Accuracy: \", acc_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN #2: LeNet5 with Data Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=5, # used, Int. Degree range for random rotations.\n",
    "    width_shift_range=0.1, # used, Float (fraction of total width). Range for random horizontal shifts.\n",
    "    height_shift_range=0.1, # used,  Float (fraction of total height). Range for random vertical shifts.\n",
    "    shear_range=0., # Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
    "    zoom_range=0.,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "117/117 [==============================] - 6s - loss: 0.1526 - acc: 0.9608 - val_loss: 0.1251 - val_acc: 0.9725\n",
      "Epoch 2/10\n",
      "117/117 [==============================] - 4s - loss: 0.1460 - acc: 0.9649 - val_loss: 0.1158 - val_acc: 0.9737\n",
      "Epoch 3/10\n",
      "117/117 [==============================] - 4s - loss: 0.1511 - acc: 0.9611 - val_loss: 0.1413 - val_acc: 0.9620\n",
      "Epoch 4/10\n",
      "117/117 [==============================] - 4s - loss: 0.1474 - acc: 0.9629 - val_loss: 0.1172 - val_acc: 0.9743\n",
      "Epoch 5/10\n",
      "117/117 [==============================] - 4s - loss: 0.1462 - acc: 0.9609 - val_loss: 0.1282 - val_acc: 0.9690\n",
      "Epoch 6/10\n",
      "117/117 [==============================] - 4s - loss: 0.1440 - acc: 0.9639 - val_loss: 0.1243 - val_acc: 0.9713\n",
      "Epoch 7/10\n",
      "117/117 [==============================] - 4s - loss: 0.1420 - acc: 0.9631 - val_loss: 0.1136 - val_acc: 0.9753\n",
      "Epoch 8/10\n",
      "117/117 [==============================] - 4s - loss: 0.1411 - acc: 0.9629 - val_loss: 0.3713 - val_acc: 0.8947\n",
      "Epoch 9/10\n",
      "117/117 [==============================] - 4s - loss: 0.1388 - acc: 0.9656 - val_loss: 0.1036 - val_acc: 0.9760\n",
      "Epoch 10/10\n",
      "117/117 [==============================] - 4s - loss: 0.1423 - acc: 0.9644 - val_loss: 0.3231 - val_acc: 0.9053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cd438c09e8>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn2_de = Model(inputs=input_holder,outputs=dense2_2)\n",
    "\n",
    "# Let's train the model \n",
    "cnn2_de.compile(loss='binary_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "                optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "datagen.fit(X_sub_train)\n",
    "cnn2_de.fit_generator(datagen.flow(X_sub_train, y_sub_train_ohe, batch_size=128), \n",
    "                      steps_per_epoch=int(len(X_sub_train)/128), # how many generators to go through per epoch\n",
    "                      epochs=10, verbose=1,\n",
    "                      validation_data=(X_sub_test,y_sub_test_ohe),\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=4)]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.717\n",
      "Accuracy:  0.905333333333\n"
     ]
    }
   ],
   "source": [
    "yhat_cnn = np.argmax(cnn2_de.predict(X_sub_test), axis=1)\n",
    "rec_cnn = mt.recall_score(y_sub_test,yhat_cnn)\n",
    "acc_cnn = mt.accuracy_score(y_sub_test,yhat_cnn)\n",
    "print(\"Recall: \", rec_cnn)\n",
    "print(\"Accuracy: \", acc_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN #1 Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "117/117 [==============================] - 7s - loss: -5.1721 - acc: 0.6655 - val_loss: -5.3147 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "117/117 [==============================] - 4s - loss: -5.3452 - acc: 0.6656 - val_loss: -5.3393 - val_acc: 0.6667\n",
      "Epoch 3/10\n",
      "117/117 [==============================] - 4s - loss: -5.3355 - acc: 0.6672 - val_loss: -5.3507 - val_acc: 0.6667\n",
      "Epoch 4/10\n",
      "117/117 [==============================] - 4s - loss: -5.3299 - acc: 0.6680 - val_loss: -5.3568 - val_acc: 0.6667\n",
      "Epoch 5/10\n",
      "117/117 [==============================] - 4s - loss: -5.3210 - acc: 0.6689 - val_loss: -5.3598 - val_acc: 0.6667\n",
      "Epoch 6/10\n",
      "117/117 [==============================] - 4s - loss: -5.3884 - acc: 0.6648 - val_loss: -5.3617 - val_acc: 0.6667\n",
      "Epoch 7/10\n",
      "117/117 [==============================] - 4s - loss: -5.3323 - acc: 0.6684 - val_loss: -5.3634 - val_acc: 0.6667\n",
      "Epoch 8/10\n",
      "117/117 [==============================] - 4s - loss: -5.3543 - acc: 0.6672 - val_loss: -5.3650 - val_acc: 0.6667\n",
      "Epoch 9/10\n",
      "117/117 [==============================] - 4s - loss: -5.3723 - acc: 0.6661 - val_loss: -5.3655 - val_acc: 0.6667\n",
      "Epoch 10/10\n",
      "117/117 [==============================] - 4s - loss: -5.4039 - acc: 0.6642 - val_loss: -5.3671 - val_acc: 0.6667\n",
      "Wall time: 54.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers import average \n",
    "from keras.models import Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "l2_lambda = 0.0001\n",
    "\n",
    "num_ensembles = 5\n",
    "\n",
    "input_holder = Input(shape=(w, h, 3))\n",
    "\n",
    "branches = []\n",
    "for _ in range(num_ensembles):\n",
    "\n",
    "    conv1 = Conv2D(filters=32,\n",
    "                   input_shape = (w,h,1),\n",
    "                   kernel_size=(3,3),\n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda),\n",
    "                   padding='same', \n",
    "                   activation='relu')(input_holder)\n",
    "    \n",
    "    max1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv1)\n",
    "\n",
    "    conv2 = Conv2D(filters=32,\n",
    "                   kernel_size=(3,3),\n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda),\n",
    "                   padding='same', \n",
    "                   activation='relu')(max1)\n",
    "    \n",
    "    max2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv2)\n",
    "\n",
    "\n",
    "    # add one layer on flattened output\n",
    "    drop1 = Dropout(0.25)(max2) # add some dropout for regularization after conv layers\n",
    "    flat1 = Flatten()(drop1)\n",
    "    dense1 = Dense(128, \n",
    "                  activation='relu',\n",
    "                  kernel_initializer='he_uniform',\n",
    "                  kernel_regularizer=l2(l2_lambda)\n",
    "                )(flat1)\n",
    "    drop2 = Dropout(0.5)(dense1) # add some dropout for regularization, again!\n",
    "    dense2 = Dense(NUM_CLASSES, \n",
    "                  activation='sigmoid', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  kernel_regularizer=l2(l2_lambda)\n",
    "                 )(drop2)\n",
    "    \n",
    "    # now add this branch onto the master list\n",
    "    branches.append(dense2)\n",
    "\n",
    "# that's it, we just need to average the results\n",
    "ave1 = average(branches)\n",
    "\n",
    "# here is the secret sauce for setting the network using the \n",
    "#   Model API:\n",
    "cnn_ens1 = Model(inputs=input_holder,outputs=ave1)\n",
    "\n",
    "# Let's train the model \n",
    "cnn_ens1.compile(loss=w_categorical_crossentropy, # 'categorical_crossentropy' 'mean_squared_error'\n",
    "                optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# the flow method yields batches of images indefinitely, with the given transofmrations\n",
    "cnn_ens1.fit_generator(datagen.flow(X_sub_train, y_sub_train_ohe, batch_size=128), \n",
    "                      steps_per_epoch=int(len(X_sub_train)/128), # how many generators to go through per epoch\n",
    "                      epochs=10, verbose=1,\n",
    "                      validation_data=(X_sub_test,y_sub_test_ohe),\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.0\n",
      "Accuracy:  0.666666666667\n"
     ]
    }
   ],
   "source": [
    "yhat_cnn = np.argmax(cnn_ens1.predict(X_sub_test), axis=1)\n",
    "rec_cnn = mt.recall_score(y_sub_test,yhat_cnn)\n",
    "acc_cnn = mt.accuracy_score(y_sub_test,yhat_cnn)\n",
    "print(\"Recall: \", rec_cnn)\n",
    "print(\"Accuracy: \", acc_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.991\n",
      "Accuracy:  0.973\n"
     ]
    }
   ],
   "source": [
    "yhat_cnn = np.argmax(cnn1.predict(X_sub_test), axis=1)\n",
    "rec_cnn = mt.recall_score(y_sub_test,yhat_cnn)\n",
    "acc_cnn = mt.accuracy_score(y_sub_test,yhat_cnn)\n",
    "print(\"Recall: \", rec_cnn)\n",
    "print(\"Accuracy: \", acc_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OKAY BOIS WE JUST TRYNA GET THAT CROSS VALIDATION WORKIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.960151802657\n",
      "Accuracy:  0.966666666667\n",
      "-----------------------\n",
      "Recall:  0.977505112474\n",
      "Accuracy:  0.979333333333\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "l2_lambda = 0.0001\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_folds = 10\n",
    "\n",
    "model_list1 = []\n",
    "yhat_list1 = []\n",
    "y_test_list1 = []\n",
    "\n",
    "skf = KFold(n_splits=num_folds).split(X_sub_train, y_sub_train_ohe)\n",
    "for k, (train_indices, test_indices) in enumerate(skf):\n",
    "    cv_X_train = np.array(X_sub_train[train_indices])\n",
    "    cv_y_train = np.array(y_sub_train[train_indices])\n",
    "    cv_y_train_ohe = np.array(y_sub_train_ohe[train_indices])\n",
    "    \n",
    "    cv_X_test = np.array(X_sub_train[test_indices])\n",
    "    cv_y_test = np.array(y_sub_train[test_indices])\n",
    "    cv_y_test_ohe = np.array(y_sub_train_ohe[test_indices])\n",
    "    \n",
    "    \n",
    "    input_holder = Input(shape=(w, h, 3))\n",
    "    conv1 = Conv2D(filters=32,\n",
    "                   input_shape = (w,h,1),\n",
    "                   kernel_size=(3,3),\n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda),\n",
    "                   padding='same', \n",
    "                   activation='relu')(input_holder)\n",
    "\n",
    "    max1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv1)\n",
    "\n",
    "    conv2 = Conv2D(filters=32,\n",
    "                   kernel_size=(3,3),\n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda),\n",
    "                   padding='same', \n",
    "                   activation='relu')(max1)\n",
    "\n",
    "    max2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv2)\n",
    "\n",
    "\n",
    "    # add one layer on flattened output\n",
    "    drop1 = Dropout(0.25)(max2) # add some dropout for regularization after conv layers\n",
    "    flat1 = Flatten()(drop1)\n",
    "    dense1 = Dense(128, \n",
    "                  activation='relu',\n",
    "                  kernel_initializer='he_uniform',\n",
    "                  kernel_regularizer=l2(l2_lambda)\n",
    "                )(flat1)\n",
    "    drop2 = Dropout(0.5)(dense1) # add some dropout for regularization, again!\n",
    "    dense2_1 = Dense(2, \n",
    "                  activation='sigmoid', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  kernel_regularizer=l2(l2_lambda)\n",
    "                 )(drop2)\n",
    "    cnn1_de = Model(inputs=input_holder,outputs=dense2_1)\n",
    "\n",
    "    # Let's train the model \n",
    "    cnn1_de.compile(loss='binary_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "                    optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "                    metrics=['accuracy'])\n",
    "    datagen.fit(cv_X_train)\n",
    "    cnn1_de.fit_generator(datagen.flow(cv_X_train, cv_y_train_ohe, batch_size=128), \n",
    "                          steps_per_epoch=int(len(cv_X_train)/128), # how many generators to go through per epoch\n",
    "                          epochs=10, verbose=0,\n",
    "                          validation_data=(cv_X_test, cv_y_test_ohe)\n",
    "                          #callbacks=[EarlyStopping(monitor='val_loss', patience=4)]\n",
    "                         )\n",
    "    \n",
    "    yhat = np.argmax(cnn1_de.predict(cv_X_test), axis=1)\n",
    "    rec_cnn = mt.recall_score(cv_y_test,yhat)\n",
    "    acc_cnn = mt.accuracy_score(cv_y_test,yhat)\n",
    "    print(\"Recall: \", rec_cnn)\n",
    "    print(\"Accuracy: \", acc_cnn)\n",
    "    print(\"-----------------------\")\n",
    "    model_list1.append(cnn1_de)\n",
    "    yhat_list1.append(yhat)\n",
    "    y_test_list1.append(cv_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Luke's\" loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import functools\n",
    "from itertools import product\n",
    "import tensorflow as tf\n",
    "weights=np.array([[1,0],[-1,1]])\n",
    "def w_categorical_crossentropy(y_true, y_pred):\n",
    "    nb_cl = len(weights)\n",
    "    final_mask = K.zeros_like(y_pred[:, 0])\n",
    "    y_pred_max = K.max(y_pred, axis=1)\n",
    "    y_pred_max = tf.expand_dims(y_pred_max, 1)\n",
    "    y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "    for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "\n",
    "        final_mask += (K.cast(weights[c_t, c_p],K.floatx()) * K.cast(y_pred_max_mat[:, c_p] ,K.floatx())* K.cast(y_true[:, c_t],K.floatx()))\n",
    "    return K.categorical_crossentropy(y_pred, y_true) * final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "117/117 [==============================] - 7s - loss: 0.4956 - acc: 0.8506 - val_loss: 0.3133 - val_acc: 0.9228\n",
      "Epoch 2/10\n",
      "117/117 [==============================] - 4s - loss: 0.3347 - acc: 0.9074 - val_loss: 0.2687 - val_acc: 0.9452\n",
      "Epoch 3/10\n",
      "117/117 [==============================] - 4s - loss: 0.2796 - acc: 0.9316 - val_loss: 0.2065 - val_acc: 0.9652\n",
      "Epoch 4/10\n",
      "117/117 [==============================] - 4s - loss: 0.2488 - acc: 0.9416 - val_loss: 0.1971 - val_acc: 0.9625\n",
      "Epoch 5/10\n",
      "117/117 [==============================] - 4s - loss: 0.2271 - acc: 0.9504 - val_loss: 0.1820 - val_acc: 0.9670\n",
      "Epoch 6/10\n",
      "117/117 [==============================] - 4s - loss: 0.2182 - acc: 0.9506 - val_loss: 0.1932 - val_acc: 0.9613\n",
      "Epoch 7/10\n",
      "117/117 [==============================] - 4s - loss: 0.2043 - acc: 0.9564 - val_loss: 0.1912 - val_acc: 0.9595\n",
      "Epoch 8/10\n",
      "117/117 [==============================] - 4s - loss: 0.1960 - acc: 0.9601 - val_loss: 0.1597 - val_acc: 0.9707\n",
      "Epoch 9/10\n",
      "117/117 [==============================] - 4s - loss: 0.1879 - acc: 0.9609 - val_loss: 0.2236 - val_acc: 0.9407\n",
      "Epoch 10/10\n",
      "117/117 [==============================] - 4s - loss: 0.1815 - acc: 0.9619 - val_loss: 0.1752 - val_acc: 0.9628\n",
      "Wall time: 51.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers import average \n",
    "from keras.models import Input, Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "l2_lambda = 0.0001\n",
    "\n",
    "num_ensembles = 5\n",
    "\n",
    "input_holder = Input(shape=(w, h, 3))\n",
    "\n",
    "branches = []\n",
    "for _ in range(num_ensembles):\n",
    "\n",
    "    conv1 = Conv2D(filters=32,\n",
    "                   input_shape = (w,h,1),\n",
    "                   kernel_size=(3,3),\n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda),\n",
    "                   padding='same', \n",
    "                   activation='relu')(input_holder)\n",
    "    \n",
    "    max1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv1)\n",
    "\n",
    "    conv2 = Conv2D(filters=32,\n",
    "                   kernel_size=(3,3),\n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda),\n",
    "                   padding='same', \n",
    "                   activation='relu')(max1)\n",
    "    \n",
    "    max2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv2)\n",
    "\n",
    "\n",
    "    # add one layer on flattened output\n",
    "    drop1 = Dropout(0.25)(max2) # add some dropout for regularization after conv layers\n",
    "    flat1 = Flatten()(drop1)\n",
    "    dense1 = Dense(128, \n",
    "                  activation='relu',\n",
    "                  kernel_initializer='he_uniform',\n",
    "                  kernel_regularizer=l2(l2_lambda)\n",
    "                )(flat1)\n",
    "    drop2 = Dropout(0.5)(dense1) # add some dropout for regularization, again!\n",
    "    dense2 = Dense(NUM_CLASSES, \n",
    "                  activation='sigmoid', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  kernel_regularizer=l2(l2_lambda)\n",
    "                 )(drop2)\n",
    "    \n",
    "    # now add this branch onto the master list\n",
    "    branches.append(dense2)\n",
    "\n",
    "# that's it, we just need to average the results\n",
    "ave = average(branches)\n",
    "\n",
    "# here is the secret sauce for setting the network using the \n",
    "#   Model API:\n",
    "cnn_ens2 = Model(inputs=input_holder,outputs=ave)\n",
    "\n",
    "# Let's train the model \n",
    "cnn_ens2.compile(loss='binary_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "                optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# the flow method yields batches of images indefinitely, with the given transofmrations\n",
    "cnn_ens2.fit_generator(datagen.flow(X_sub_train, y_sub_train_ohe, batch_size=128), \n",
    "                      steps_per_epoch=int(len(X_sub_train)/128), # how many generators to go through per epoch\n",
    "                      epochs=10, verbose=1,\n",
    "                      validation_data=(X_sub_test,y_sub_test_ohe),\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=4)]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.945\n",
      "Accuracy:  0.973\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "yhat_cnn = np.argmax(cnn_ens.predict(X_sub_test), axis=1)\n",
    "rec_cnn = mt.recall_score(y_sub_test,yhat_cnn)\n",
    "acc_cnn = mt.accuracy_score(y_sub_test,yhat_cnn)\n",
    "print(\"Recall: \", rec_cnn)\n",
    "print(\"Accuracy: \", acc_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.896\n",
      "Accuracy:  0.963\n"
     ]
    }
   ],
   "source": [
    "yhat_cnn = np.argmax(cnn_ens2.predict(X_sub_test), axis=1)\n",
    "rec_cnn = mt.recall_score(y_sub_test,yhat_cnn)\n",
    "acc_cnn = mt.accuracy_score(y_sub_test,yhat_cnn)\n",
    "print(\"Recall: \", rec_cnn)\n",
    "print(\"Accuracy: \", acc_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "#SVG(model_to_dot(cnn2_de).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "117/117 [==============================] - 12s - loss: 0.7890 - acc: 0.8268 - val_loss: 3.2607 - val_acc: 0.6667\n",
      "Epoch 2/10\n",
      "117/117 [==============================] - 6s - loss: 0.5357 - acc: 0.8921 - val_loss: 0.7826 - val_acc: 0.6770\n",
      "Epoch 3/10\n",
      "117/117 [==============================] - 6s - loss: 0.4470 - acc: 0.9091 - val_loss: 0.6236 - val_acc: 0.8488\n",
      "Epoch 4/10\n",
      "117/117 [==============================] - 6s - loss: 0.3921 - acc: 0.9215 - val_loss: 0.5495 - val_acc: 0.9023\n",
      "Epoch 5/10\n",
      "117/117 [==============================] - 6s - loss: 0.3479 - acc: 0.9292 - val_loss: 0.3835 - val_acc: 0.9307\n",
      "Epoch 6/10\n",
      "117/117 [==============================] - 6s - loss: 0.3261 - acc: 0.9345 - val_loss: 0.3982 - val_acc: 0.93550.\n",
      "Epoch 7/10\n",
      "117/117 [==============================] - 6s - loss: 0.3108 - acc: 0.9324 - val_loss: 0.4217 - val_acc: 0.9033\n",
      "Epoch 8/10\n",
      "117/117 [==============================] - 6s - loss: 0.2887 - acc: 0.9386 - val_loss: 0.3146 - val_acc: 0.9483\n",
      "Epoch 9/10\n",
      "117/117 [==============================] - 6s - loss: 0.2762 - acc: 0.9427 - val_loss: 0.3396 - val_acc: 0.9198\n",
      "Epoch 10/10\n",
      "117/117 [==============================] - 6s - loss: 0.2660 - acc: 0.9417 - val_loss: 0.2500 - val_acc: 0.9578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cf0a714518>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "l2_lambda = 0.0001\n",
    "\n",
    "num_ensembles = 10\n",
    "input_holder = Input(shape=(w, h, 3))\n",
    "branches_le5 = []\n",
    "for _ in range(num_ensembles):\n",
    "    conv1 = Conv2D(filters=6,kernel_size=(5,5),\n",
    "                   input_shape = (img_wh,img_wh,1), \n",
    "                   padding='valid', \n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda))(input_holder)\n",
    "    max1 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv1)\n",
    "    batch1 = BatchNormalization()(max1)\n",
    "    activ1 = Activation(\"sigmoid\")(batch1)\n",
    "\n",
    "    conv2 = Conv2D(filters=16,kernel_size=(5,5), \n",
    "                   padding='valid', \n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda))(activ1)\n",
    "    max2 = MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\")(conv2)\n",
    "    batch2 = BatchNormalization()(max2)\n",
    "    activ2 = Activation(\"sigmoid\")(batch2)\n",
    "\n",
    "    conv3 = Conv2D(filters=120,kernel_size=(1,1), \n",
    "                   padding='valid', \n",
    "                   kernel_initializer='he_uniform', \n",
    "                   kernel_regularizer=l2(l2_lambda))(activ2)\n",
    "\n",
    "\n",
    "    # add one layer on flattened output\n",
    "    #drop1 = Dropout(0.25)(max2) # add some dropout for regularization after conv layers\n",
    "    flat1 = Flatten()(conv3)\n",
    "    dense1 = Dense(128, \n",
    "                  activation='relu',\n",
    "                  kernel_initializer='he_uniform',\n",
    "                  kernel_regularizer=l2(l2_lambda)\n",
    "                )(flat1)\n",
    "    drop2 = Dropout(0.5)(dense1) # add some dropout for regularization, again!\n",
    "    dense2 = Dense(NUM_CLASSES, \n",
    "                  activation='sigmoid', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  kernel_regularizer=l2(l2_lambda)\n",
    "                 )(drop2)\n",
    "    branches_le5.append(dense2)\n",
    "\n",
    "# that's it, we just need to average the results\n",
    "ave_le5 = average(branches_le5)   \n",
    "\n",
    "\n",
    "cnn_le5 = Model(inputs=input_holder,outputs=ave_le5)\n",
    "\n",
    "# Let's train the model \n",
    "cnn_le5.compile(loss='binary_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "                optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# the flow method yields batches of images indefinitely, with the given transofmrations\n",
    "cnn_le5.fit_generator(datagen.flow(X_sub_train, y_sub_train_ohe, batch_size=128), \n",
    "                      steps_per_epoch=int(len(X_sub_train)/128), # how many generators to go through per epoch\n",
    "                      epochs=10, verbose=1,\n",
    "                      validation_data=(X_sub_test,y_sub_test_ohe),\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=4)]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_37_input to have shape (None, 32, 32, 1) but got array with shape (2954, 32, 32, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-5339c6797cad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_sub_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test_ohe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                   \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                  )\n",
      "\u001b[1;32mc:\\users\\ovall\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ovall\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[0;32m   1115\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ovall\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ovall\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[0;32m   1775\u001b[0m                                  str(validation_data))\n\u001b[0;32m   1776\u001b[0m             val_x, val_y, val_sample_weights = self._standardize_user_data(\n\u001b[1;32m-> 1777\u001b[1;33m                 val_x, val_y, val_sample_weight)\n\u001b[0m\u001b[0;32m   1778\u001b[0m             \u001b[0mval_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1779\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ovall\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1232\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1235\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1236\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ovall\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    138\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    141\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_37_input to have shape (None, 32, 32, 1) but got array with shape (2954, 32, 32, 3)"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2 \n",
    "l2_lambda = 0.0001\n",
    "img_wh = 32\n",
    "\n",
    "# Use Kaiming He to regularize ReLU layers: https://arxiv.org/pdf/1502.01852.pdf\n",
    "# Use Glorot/Bengio for linear/sigmoid/softmax: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf \n",
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Conv2D(filters=32,\n",
    "               input_shape = (img_wh,img_wh,1),\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu')) # more compact syntax\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "\n",
    "cnn.add(Conv2D(filters=32,\n",
    "               kernel_size=(3,3),\n",
    "               kernel_initializer='he_uniform', \n",
    "               kernel_regularizer=l2(l2_lambda),\n",
    "               padding='same', \n",
    "               activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_last\"))\n",
    "    \n",
    "\n",
    "# add one layer on flattened output\n",
    "cnn.add(Dropout(0.25)) # add some dropout for regularization after conv layers\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, \n",
    "              activation='relu',\n",
    "              kernel_initializer='he_uniform',\n",
    "              kernel_regularizer=l2(l2_lambda)\n",
    "       ))\n",
    "cnn.add(Dropout(0.5)) # add some dropout for regularization, again!\n",
    "cnn.add(Dense(NUM_CLASSES, \n",
    "              activation='softmax', \n",
    "              kernel_initializer='glorot_uniform',\n",
    "              kernel_regularizer=l2(l2_lambda)\n",
    "             ))\n",
    "\n",
    "# Let's train the model \n",
    "cnn.compile(loss='categorical_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "              optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# the flow method yields batches of images indefinitely, with the given transofmrations\n",
    "cnn.fit_generator(datagen.flow(X_subtrain, y_train_ohe, batch_size=128), \n",
    "                  steps_per_epoch=int(len(X_train)/128), # how many generators to go through per epoch\n",
    "                  epochs=50, verbose=1,\n",
    "                  validation_data=(X_sub_test,y_test_ohe),\n",
    "                  callbacks=[EarlyStopping(monitor='val_loss', patience=2)]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
